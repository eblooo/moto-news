# AI Agents Configuration
# Copy to agents.yaml and adjust for your environment

# Default LLM provider for user_agent and site_assessor
llm:
  provider: openrouter                                    # "openrouter" or "ollama"
  # openrouter_api_key: set via OPENROUTER_API_KEY env var
  user_model: "meta-llama/llama-3.3-70b-instruct:free"   # 128K ctx, best instruction following
  coder_model: "google/gemma-3-27b-it:free"              # 131K ctx, good reasoning/structured output
  admin_model: "nousresearch/hermes-3-llama-3.1-405b:free"  # 131K ctx, strongest code gen

# Ollama config â€” still used by admin_agent (and as fallback when LLM_PROVIDER=ollama)
ollama:
  host: http://localhost:11434       # Local dev; use http://ollama-host-svc:11434 in K8s
  user_model: llama3.2:3b            # Fast, supports tool calling (ReAct agent)
  admin_model: llama3.2:3b           # Supports tool calling (LangGraph agent)
  coder_model: qwen2.5-coder:7b     # Better reasoning for analysis tasks
  temperature: 0.35
  num_ctx: 8192

github:
  # token: set via GITHUB_TOKEN env var
  repo: KlimDos/my-blog
  discussions_category: For Developers

site:
  url: https://blog.alimov.top
  # repo_path: set via BLOG_REPO_PATH env var

schedule_interval_minutes: 60
log_level: INFO
